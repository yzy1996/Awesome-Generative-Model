- 归一化图像输入到（-1，1）之间；Generator最后一层使用tanh激活函数
- 生成器的Loss采用：min (log 1-D)。因为原始的生成器Loss存在梯度消失问题；训练生成器的时候，考虑反转标签，real=fake, fake=real
- 不要在均匀分布上采样，应该在高斯分布上采样
- 一个Mini-batch里面必须只有正样本，或者负样本。不要混在一起；如果用不了Batch Norm，可以用Instance Norm
- 避免稀疏梯度，即少用ReLU，MaxPool。可以用LeakyReLU替代ReLU，下采样可以用Average Pooling或者Convolution + stride替代。上采样可以用PixelShuffle, ConvTranspose2d + stride
- 平滑标签或者给标签加噪声；平滑标签，即对于正样本，可以使用0.7-1.2的随机数替代；对于负样本，可以使用0-0.3的随机数替代。 给标签加噪声：即训练判别器的时候，随机翻转部分样本的标签。
- 如果可以，请用DCGAN或者混合模型：KL+GAN，VAE+GAN。
- 使用LSGAN，WGAN-GP
- Generator使用Adam，Discriminator使用SGD
- 尽快发现错误；比如：判别器Loss为0，说明训练失败了；如果生成器Loss稳步下降，说明判别器没发挥作用
- 不要试着通过比较生成器，判别器Loss的大小来解决训练过程中的模型坍塌问题。比如： While Loss D > Loss A: Train D While Loss A > Loss D: Train A
- 如果有标签，请尽量利用标签信息来训练
- 给判别器的输入加一些噪声，给G的每一层加一些人工噪声。
- 多训练判别器，尤其是加了噪声的时候
- 对于生成器，在训练，测试的时候使用Dropout
Speed up Diffusion Model Sampling



当我们设计的拆分步数多了之后，马尔可夫链的采样过程是非常缓慢的，原文中说采样50k张32*32的图片需要20小时，



一个最简单的办法就是跳跃式地采样

[Improved Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2102.09672)






## DDIM

- “consistency” property
- DDIM can do semantically meaningful interpolation in the latent variable.



**首先回顾DDPM，以及提炼出其中的重要公式。**



Forward Process: a Markov chain with Gaussian transitions parameterized by a decreasing sequence $\alpha_{1:T}$.
$$
q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0) := \prod_{t=1}^T q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1}), \ \text{ where } \ q\left(\boldsymbol{x}_t | \boldsymbol{x}_{t-1}\right) := \mathcal{N}\left(\sqrt{\frac{\alpha_t}{\alpha_{t-1}}} \boldsymbol{x}_{t-1}, (1-\frac{\alpha_t}{\alpha_{t-1}}) \mathbf{I}\right)
$$
Reverse Process: 
$$
p_\theta(\boldsymbol{x}_{0}) = \int p_\theta(\boldsymbol{x}_{0: T}) d \boldsymbol{x}_{1: T}, \ \text{ where } \ p_\theta(\boldsymbol{x}_{0:T}) = p(\boldsymbol{x}_T)\prod_{t=1}^Tp_\theta(\boldsymbol{x}_{t-1}|\boldsymbol{x}_{t})
$$
优化过程是 最大似然估计
$$
\max_\theta \ L(\theta) := \mathbb{E}_{q(\boldsymbol{x}_0)}\left[\log p_\theta(\boldsymbol{x}_{0}) \right]
$$
对于每一个样本 $\boldsymbol{x}_0$，我们都可以将求解上式转化为求解ELBO
$$
\max_\theta \ \log p_\theta(\boldsymbol{x}_{0}) \ge \max_\theta \ \mathbb{E}_{q(\boldsymbol{x}_{1: T} | \boldsymbol{x}_0)}\left[\log \frac{p_\theta(\boldsymbol{x}_{0: T})}{q(\boldsymbol{x}_{1: T} | \boldsymbol{x}_0)}\right] 
$$
最终我们是化简成了
$$
\min_\theta \mathbb{E}_{q(\boldsymbol{x}_0)} \left[\sum_{t=1}^T \gamma_t \mathbb{E}_{\boldsymbol{\varepsilon}_t \sim \mathcal{N}(0, I)}\left[\|f_\theta(\boldsymbol{x}_t, t)  - \boldsymbol{\varepsilon}_t\|_2^2\right]\right], \text{ where } \boldsymbol{x}_t =\sqrt{a_t} \boldsymbol{x}_0 + \sqrt{1- a_t}\boldsymbol{\varepsilon}_t
$$
其中 $\gamma_t$ 是非负系数，和 $\alpha_t$ 有关。在DDPM论文中设置为1，



---



通过上面的过程我们可以发现，正向过程本来建模的是 $q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)$，但优化目标里并没有，只用到了 $q(\boldsymbol{x}_t|\boldsymbol{x}_0)$



之前有一个关系是
$$
\boldsymbol{x}_t=\sqrt{\alpha_t} \boldsymbol{x}_0+\sqrt{1-\alpha_t} \boldsymbol{\varepsilon}_t, \quad \text { where } \quad \boldsymbol{\varepsilon}_t \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})
$$
给定 $\boldsymbol{x}_0, t$ 可以得到 $x_t$，然后网络模型预测了 $\hat{\boldsymbol{\varepsilon}}_t$ 

将上式颠倒，我们可以得到
$$
g_\theta(\boldsymbol{x}_t, t) = \hat{\boldsymbol{x}}_0 := \frac{\boldsymbol{x}_t - \sqrt{1-\alpha_t} \hat{\boldsymbol{\varepsilon}}_t}{\sqrt{\alpha_t}}
$$
这样我们就可以表示
$$
p_\theta(\boldsymbol{x}_{t-1} | \boldsymbol{x}_t)= \begin{cases}\mathcal{N}\left(g_\theta\left(\boldsymbol{x}_1, 1\right), \sigma_1^2 \boldsymbol{I}\right) & \text { if } t=1 \\ q_\sigma\left(\boldsymbol{x}_{t-1} |\boldsymbol{x}_t, g_\theta\left(\boldsymbol{x}_t, t\right)\right) & \text { otherwise }\end{cases}
$$








## Latent Diffusion Model (LDM)

> 在隐空间操作，而不是像素空间。

$$
\begin{aligned}
    \boldsymbol{x}_{t-1} &=\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0+\sqrt{1-\bar{\alpha}_{t-1}} \boldsymbol{\varepsilon}_{t-1} \\
    &=\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0+\sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2} \boldsymbol{\varepsilon}_t+\sigma_t \boldsymbol{\varepsilon} \\
    &=\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0+\sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2} \frac{\boldsymbol{x}_t-\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0}{\sqrt{1-\bar{\alpha}_t}}+\sigma_t \boldsymbol{\varepsilon}
\end{aligned}
$$






$$
q_\sigma\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2} \frac{\mathbf{x}_t-\sqrt{\bar{\alpha}_t} \mathbf{x}_0}{\sqrt{1-\bar{\alpha}_t}}, \sigma_t^2 \mathbf{I}\right)
$$

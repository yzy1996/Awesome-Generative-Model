#! https://zhuanlan.zhihu.com/p/594007789
本文将详细介绍经验贝叶斯估计，引出 Robbins， James-Stein，Tweedie 三种估计方法 (给出详细证明)，最后以一个经典的缺失物种的估计案例带大家回顾 (很精彩不要错过!)。另外关于基本的参数估计，可以去参看我的另一篇[博客](https://zhuanlan.zhihu.com/p/592423240)。

> 为什么写这篇文章呢，起因是在推导从SDE角度理解Diffusion Model时，在参考资料中出现了一个Tweedie’s formula，带着很多的疑问进而探究了一下经验贝叶斯估计的全貌。



贝叶斯估计的问题定义为根据一些观测数据 $x$ 来估计未知参数 $\theta$，用一个损失函数来衡量估计的准确性，如果用均方误差(MSE)来估计的话，我们将问题建模为：
$$
L = \mathbb{E}[(\hat{\theta}(x) -\theta)^2]
$$
而这样等价于求解后验分布的均值：
$$
\hat{\theta}(x) = \mathbb{E}[\theta | x] = \int \theta \ p(\theta | x) \ \mathrm{d} \theta
$$
这被称为最小均方误差估计器 minimum mean square error (MMSE) estimator。

上述估计过程中，我们需要知道后验概率 $p(\theta | x)$，而它是利用贝叶斯定理 $p(\theta | x) = \frac{p(x|\theta)p(\theta)}{p(x)}$ 求出的；因此我们需要知道似然函数 $p(x|\theta)$ 以及先验分布 $p(\theta)$ 。


首先让我们回顾一下**经典贝叶斯**参数估计。首先有一组观测样本 $x = \{x_1, x_2, \dots, x_n\}$，这一组样本都是从条件概率密度为 $p(x|\theta)$ 的分布中采样出来的。我们是在已知先验 $p(\theta)$ 参数的前提下，估计目标参数 $\hat{\theta}$。而**经验贝叶斯** (Empirical Bayes)里，先验分布的参数是没有提前给定的，也是要通过观测数据去估计。

经验贝叶斯方法可以被看成是**分层贝叶斯模型**：在一个两阶段的分层贝叶斯模型中，我们的观测数据 $x = \{x_1, x_2, \dots, x_n\}$ 是从一组未知的参数 $\theta = \{\theta_1, \theta_2, \dots, \theta_n\}$ 中根据 $p(x|\theta)$ 生成的，注意每一个样本 $x_i$ 是只对应参数 $\theta_i$ 的，这里不像经典贝叶斯估计里，对于一个参数我们有多个观测数据，而是对于一个参数我们只有一个观测数据； $\theta$ 是从 $p(\theta|\eta)$ 中获取的，其中 $p(\eta)$ 是非参数的分布，我们也可以直接表示为 $p(\theta)$。目标是估计 $\theta_1, \theta_2, \dots, \theta_n$。

> 看一个更具体的以高斯分布为例。我们是先从分布 $\mathcal{N}(0, \sigma^2)$ 采样一组 $\{\mu_1, \mu_2, \dots, \mu_n\}$，再分别从分布 $\mathcal{N}(\mu_i, 1)$ 采样一个 $x_i$，构成一组观测 $\{x_1, x_2, \dots, x_n\}$，相当于二次采样。

**另一种理解**的方式是将它看成是一个多元混合分布呢，我们采样一个样本，但它有多个维度，每个维度是服从一个分布的，所以不同维度就服从不同的分布。

> 如果要统一起来，方便符号表示的话，记单一样本为 $x_i^j$ ，下标表示是第几维，上标表示是这一维的第几次采样。 

利用最大似然估计，我们很容易得知 $\theta_i = x_i$，因为对于待估计的参数 $\theta_i$ 只有一个观测样本。而如果有更多$m$个观测样本 $x_{i}^1, x_{i}^2, \dots, x_{i}^m$ 则会估计得更准确。



**直观理解就是：**经验贝叶斯需要用到辅助的经验信息，待估计参数可以通过其他相关参数进行辅助（因为这些参数是从相同的先验形式），而其他相关参数又是可以通过其他观测数据获得的，这样我们就可以使用来自其他观测数据来改进特定参数的估计性能。



接下来将会介绍三个发展历史上的重要方法。



## Robbins Estimator (罗宾斯估计)

根据前面的问题定义，在泊松分布下 $x|\theta \sim P(\theta)$，我们有：
$$
p(x|\theta) = \frac{\theta^{x}e^{-\theta}}{x!}, \quad p(x) = \int_0^\infty \frac{\theta^{x}e^{-\theta}}{x!} p(\theta) \mathrm{d} \theta, \quad p(\theta|x) = \frac{\frac{\theta^{x}e^{-\theta}}{x!}}{p(x)}p(\theta)
$$

这样可以写出后验的期望：
$$
\begin{aligned}
\mathbb{E}[\theta | x] 
&= \int_{0}^{\infty} \theta \ p(\theta|x) \mathrm{d} \theta \\
&= \int_{0}^{\infty} \theta \frac{ \frac{\theta^{x}e^{-\theta}}{x!} p(\theta)}{p(x)} \mathrm{d} \theta \\
&= \frac{\int_{0}^{\infty} \frac{\theta^{x+1}e^{-\theta}}{x!}p(\theta) \mathrm{d} \theta} {p(x)} \\
&= \frac{\int_{0}^{\infty} (x+1)\frac{\theta^{x+1}e^{-\theta}}{(x+1)!}p(\theta) \mathrm{d} \theta} {p(x)} \\
&= (x+1) \frac{\int_{0}^{\infty} \frac{\theta^{x+1}e^{-\theta}}{(x+1)!}p(\theta) \mathrm{d} \theta} {p(x)} \\
&= (x+1) \frac{p(x+1)}{p(x)} \\
\end{aligned}
$$
对于边缘分布 $p(x)$ 我们可以用 $x$ 出现的频率来估计，如果我们知道对应取值 $x_1, x_2, \dots, x_n$ 的出现频次分别为 $y_1, y_2, \dots, y_n$；那么 $p(x_i) = \frac{y_i}{n}$。



**小总结：** 我们最终得到了罗宾斯估计结果，而要用到这个结论，需要知道事件发生的频次，而且目标项只增加后一项对它的信息增强，如果目标项是最后一项其实也是无能为力的。
$$
\hat{\theta}_i^{RE} := \mathbb{E}[\theta|x] = (x_i+1)\frac{y_{i+1}}{y_i}
$$



## James-Stein Estimator (詹姆斯坦估计)

罗宾斯估计中是针对泊松分布进行的研究，詹姆斯坦估计则是针对高斯分布进行的研究。我们先假设一个简单的例子，其中 $\theta$ 是未知的，$\sigma$ 是已知的：
$$
p(x|\theta) = \mathcal{N}(\theta, 1), \quad p(\theta) = \mathcal{N}(0, \sigma^2)
$$
我们先要写出 $x$ 的边缘分布：
$$
\begin{aligned}
p(x) 
&= \int_{-\infty}^{\infty} p(x|\theta) p(\theta) \mathrm{d} \theta \\
&= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}e^{-\frac{(x-\theta)^2}{2}} \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\theta^2}{2\sigma^2}} \mathrm{d} \theta \\
&= \frac{1}{2\pi\sigma} \int_{-\infty}^{\infty} e^{-\frac{\sigma^2(x-\theta)^2 + \theta^2}{2\sigma^2}} \mathrm{d} \theta \\
&= \frac{1}{2\pi\sigma} \int_{-\infty}^{\infty} e^{-\frac{(\sigma^2+1)\theta^2 - 2\sigma^2x\theta+\sigma^2x^2}{2\sigma^2}} \mathrm{d} \theta \\
&= \frac{1}{2\pi\sigma} \sqrt{\frac{2\pi\sigma^2}{\sigma^2+1}}e^{-\frac{x^2}{2(\sigma^2+1)}} \\ 
&= \frac{1}{\sqrt{2\pi(\sigma^2+1)}}e^{-\frac{x^2}{2(\sigma^2+1)}}  \\
&= \mathcal{N}(0, \sigma^2 +1)
\end{aligned}
$$

> 其中积分部分的推导是利用了 Gaussian intergral 的一个[结论](https://en.wikipedia.org/wiki/Gaussian_integral#:~:text=An%20alternative%20form%20is) : $\int_{-\infty}^{\infty} e^{-a x^2+b x+c} d x=\sqrt{\frac{\pi}{a}} e^{\frac{b^2}{4 a}+c}$.

我们会发现这也是一个高斯分布，意味着 $x_1, x_2, \dots, x_n$ 是从分布 $\mathcal{N}(0, \sigma^2 +1)$ 中采样得到的，且与先验分布的参数无关。 同时我们可以通过这些观测样本来估计先验分布的参数 (n-1来自于无偏估计)：
$$
\hat{\sigma}^2 + 1 = \frac{1}{n-1} \sum_{i=1}^n x_i^2
$$
借由式，我们可以进一步推导后验分布：
$$
\begin{aligned}
p(\theta|x) 
&= \frac{p(x|\theta) p(\theta)}{p(x)} \\
&= \frac{\mathcal{N}(\theta, 1)\mathcal{N}(0, \sigma^2)}{\mathcal{N}(0, \sigma^2+1)} \\
&= \frac{\frac{1}{2\pi\sigma} e^{-\frac{\sigma^2(x-\theta)^2 + \theta^2}{2\sigma^2}}}{\frac{1}{\sqrt{2\pi(\sigma^2+1)}}e^{-\frac{x^2}{2(\sigma^2+1)}}} \\
&= \frac{1}{\sqrt{2\pi \frac{\sigma^2}{\sigma^2+1}}} e^{-\frac{\left(\theta - \frac{\sigma^2 x}{\sigma^2+1}\right)^2}{2\frac{\sigma^2}{\sigma^2+1}}} \\
&= \mathcal{N}(\frac{\sigma^2 x}{\sigma^2+1}, \frac{\sigma^2}{\sigma^2+1})
\end{aligned}
$$

这样后验的期望就可以表示为：
$$
\hat{\theta}^{JSE} := \mathbb{E}[\theta|x] = \frac{\sigma^2 x}{\sigma^2+1} = (1 - \frac{1}{\sigma^2+1})x
$$
上述期望的含义是估计在观测样本下参数的值。对于一个具体的观测样本 $x_i$ 和它对应的参数 $\theta_i$ ，并代入式(2)其他样本的估计，我们可以写出
$$
\hat{\theta}_i^{JSE} = (1 - \frac{n-1}{\sum_{j=1}^n x_j^2})x_i
$$

**小总结：**相较于罗宾斯估计只用到一个相关观测，詹姆斯坦估计用到了所有观测量。



接下来我们写出它的更一般形式。对于一个多元 ( $d$ 元) 高斯分布 $\mathbf{X} \sim \mathcal{N}_d(\boldsymbol{\theta}, \sigma^2 \boldsymbol{I})$，注意加粗表示他们都是向量，其中 $\boldsymbol{\theta}$ 是未知的 (待估计)，而 $\sigma$ 是已知的。$\mathbf{X} = [\mathbf{X}_1, \mathbf{X}_2, \dots, \mathbf{X}_d]$ 其中每一维 $\mathbf{X}_i$ 都是随机变量；$\mathbf{x} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_d]$ 是 $\mathbf{X}$ 的一个观测样本，也就是只采样了一次，不像通常我们采样 $n$ 个样本。

我们想根据这一个观测来估计 $\boldsymbol{\theta}$ ，采用最大似然估计，可以很容易地得到 $\hat{\boldsymbol{\theta}}^{MLE} = \mathbf{x}$ 。因为只有一次观测，那我们当然就以这个观测值作为均值了。

然后James–Stein提出了一个新的估计方法，并证明当 $d \ge 3$ 时，能够获得比最大似然估计更小的均方误差。而这个新的估计方法写作：
$$
\hat{\boldsymbol{\theta}}^{JSE} = \left(1 - \frac{(d-2)\sigma^2}{\|\mathbf{x}\|^2}\right)\mathbf{x}
$$
当然了，对于每一维变量，我们都可以分开来计算：
$$
\hat{\boldsymbol{\theta}}_i^{JSE} = \left(1 - \frac{(d-2)\sigma^2}{\|\mathbf{x}\|^2}\right)\mathbf{x}_i , \quad i=1, 2, \dots, d
$$
如果 $\sigma$ 是未知的话，我们也可以通过 $\hat{\sigma}^2 = \frac{1}{d}\sum_{i=1}^d (\mathbf{x}_i - \frac{\mathbf{x}_1+\mathbf{x}_2+\dots+\mathbf{x}_d}{d})^2$ 或者无偏估计 $\hat{\sigma}^2 = \frac{1}{d-1}\sum_{i=1}^d (\mathbf{x}_i - \frac{\mathbf{x}_1+\mathbf{x}_2+\dots+\mathbf{x}_d}{d})^2$ 来估计；以及如果我们有更多的 ($n$ 个) 观测样本 $\{\mathbf{x}^1, \mathbf{x}^2, \dots, \mathbf{x}^n\}$，( $\mathbf{x}^j$ 是 $d$ 维的)，我们可以修正为：
$$
\hat{\boldsymbol{\theta}}^{JSE} = \left(1 - \frac{(m-2)\frac{\sigma^2}{n}}{\|\bar{\mathbf{x}}\|^2}\right)\bar{\mathbf{x}}, \quad \bar{\mathbf{x}} = \frac{\mathbf{x}^1+\mathbf{x}^2+\dots+\mathbf{x}^n}{n}
$$

> 顺带提一下，括号部分被称为收缩因子 (shrinkage factor)



## Tweedie Estimator

在上一节中，我们是假设了先验 $p(\theta)$ 是服从正态分布的，可以通过观测数据估计出参数$\sigma$。如果我们不对先验进行假设呢？

我们依旧假设 $p(x|\theta) = \mathcal{N}(\theta, \sigma^2)$；同时先写出边缘分布的形式 $p(x) = \int_{-\infty}^{\infty} p(x|\theta) p(\theta) \mathrm{d} \theta$。
$$
\begin{aligned}
    \mathbb{E}[\theta|x] 
    &= \int_{-\infty}^{\infty} \theta \ p(\theta|x) \mathrm{d}\theta \\
    &= \int_{-\infty}^{\infty} \theta \ \frac{p(x|\theta) p(\theta) }{p(x)} \mathrm{d}\theta \\
    &= \frac{\int_{-\infty}^{\infty} \theta \ p(x|\theta) p(\theta) \mathrm{d}\theta}{p(x)} \\
    & = \frac{\int_{-\infty}^{\infty} \theta \ \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\theta)^2}{2\sigma^2}} p(\theta) \mathrm{d}\theta}{p(x)} \\
    &= \frac{\int_{-\infty}^{\infty} \left[\sigma^2 \frac{\theta-x}{\sigma^2} \ \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\theta)^2}{2\sigma^2}} p(\theta) + x \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\theta)^2}{2\sigma^2}} p(\theta) \right] \mathrm{d}\theta}{p(x)} \\
    &= \frac{\int_{-\infty}^{\infty} \sigma^2 \frac{\theta-x}{\sigma^2} \ \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\theta)^2}{2\sigma^2}} p(\theta) \mathrm{d}\theta + \int_{-\infty}^{\infty} x \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\theta)^2}{2\sigma^2}} p(\theta) \mathrm{d}\theta}{p(x)} \\
    &= \frac{\sigma^2\int_{-\infty}^{\infty} \ \frac{\mathrm{d}\left[\frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\theta)^2}{2\sigma^2}}\right]}{\mathrm{d} x} p(\theta) \mathrm{d}\theta + \int_{-\infty}^{\infty} x \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\theta)^2}{2\sigma^2}} p(\theta) \mathrm{d}\theta}{p(x)}\\
    &= \frac{\sigma^2 \int_{-\infty}^{\infty}\frac{\mathrm{d} p(x|\theta)}{\mathrm{d} x}  p(\theta) \mathrm{d}\theta + \int_{-\infty}^{\infty} x \ p(x|\theta) p(\theta) \mathrm{d}\theta}{p(x)} \\
    &= \frac{\sigma^2 \frac{\mathrm{d} }{\mathrm{d} x}  \int_{-\infty}^{\infty} p(x|\theta) p(\theta) \mathrm{d}\theta + x \int_{-\infty}^{\infty} \ p(x|\theta) p(\theta) \mathrm{d}\theta}{p(x)} \\ 
    &= \frac{\sigma^2 \frac{\mathrm{d} p(x)}{\mathrm{d} x}+ x p(x)}{p(x)} \\ 
    &= x + \sigma^2 \frac{\mathrm{d}}{\mathrm{d} x} \log p(x)
    \end{aligned}
$$
这里其实可以发现 后验期望与先验分布无关，我们一直保有着 $p(\theta)$ 的雏形而没有探究它的具体样子，而仅与边缘分布有关，利用样本估计边缘分布即可。因此我们可以写出Tweedie’s estimator：
$$
\hat{\theta}^{TE} = x + \sigma^2 \frac{\mathrm{d}}{\mathrm{d} x} \log p(x)
$$
> 为了提高边缘密度估计的精度，往往会假设边缘密度属于某个参数分布族。



最后再给出一些结论：

有一些结论：

- $x|\theta \sim \mathcal{N}(\theta, \sigma^2)$ ，先验是正态分布 $\theta \sim \mathcal{N}(\mu, \tau^2)$, 则后验也是正态分布，那么MSE下的贝叶斯估计为：
  $$
  \hat{\theta}(x) = \frac{\sigma^2}{\sigma^2+\tau^2} \mu+\frac{\tau^2}{\sigma^2+\tau^2} x
  $$

- $x|\theta \sim P(\theta)$，先验是Gamma分布 $\theta \sim G(a, b)$，则后验也是Gamma分布，那么：
  $$
  \hat{\theta}(X)=\frac{n \bar{X}+a}{n+b}
  $$



参考资料

https://efron.ckirby.su.domains/papers/2021EB-concepts-methods.pdf

https://zhuanlan.zhihu.com/p/142221534

https://zhuanlan.zhihu.com/p/422201078
